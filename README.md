# Awesome Video-Transformer [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Collect some Transformer with Video papers. It is based on [Awesome Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer).

If you find some overlooked papers, please open issues or pull requests (recommended).

## Papers

### Transformer original paper

- [Attention is All You Need](https://arxiv.org/abs/1706.03762) (NIPS 2017)

### Technical blog

- [English Blog] Transformers in Vision [[Link](https://davide-coccomini.medium.com/)]
- [Chinese Blog] 3W字长文带你轻松入门视觉transformer [[Link](https://zhuanlan.zhihu.com/p/308301901)]
- [Chinese Blog] Vision Transformer 超详细解读 (原理分析+代码解读) [[Link](https://zhuanlan.zhihu.com/p/348593638)]

### Survey
  - Video transformers: A survey [[paper](https://arxiv.org/abs/2201.05991)]  - 2023.02.13

### arXiv papers
- **[VideoMAE]** VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training [[paper](https://arxiv.org/abs/2203.12602)] [[code](https://github.com/MCG-NJU/VideoMAE)]
- PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers [[paper](https://arxiv.org/pdf/2111.12710.pdf)]
- **[CrossEfficientViT]** Combining EfficientNet and Vision Transformers for Video Deepfake Detection [[paper](https://arxiv.org/abs/2107.02612)] [[code](https://github.com/davide-coccomini/Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection)]
- TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? [[paper](https://arxiv.org/abs/2106.11297)] [[code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner)]
- **[ORViT]** Object-Region Video Transformers [[paper](https://arxiv.org/abs/2110.06915)] [[code](https://roeiherz.github.io/ORViT/)]
- Adaptively Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation [[paper](https://arxiv.org/abs/2110.05092)] [[code](https://github.com/lelexx/MTF-Transformer)]

- A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition [[paper](https://arxiv.org/abs/2110.01240)]
- **[3D-Transformer]** 3D-Transformer: Molecular Representation with Transformer in 3D Space [[paper](https://arxiv.org/abs/2110.01191)]
- Sparse Spatial Transformers for Few-Shot Learning [[paper](https://arxiv.org/abs/2109.12932)]
- Do Vision Transformers See Like Convolutional Neural Networks? [[paper](https://arxiv.org/abs/2108.08810)]

- **[CMT]** CMT: Convolutional Neural Networks Meet Vision Transformers [[paper](https://arxiv.org/abs/2107.06263)]

- How to train your ViT? Data, Augmentation,and Regularization in Vision Transformers [[paper](https://arxiv.org/pdf/2106.10270.pdf)]
- Video Swin Transformer [[paper](https://arxiv.org/abs/2106.13230)] [[code](https://github.com/SwinTransformer/Video-Swin-Transformer)]
- Efficient Self-supervised Vision Transformers for Representation Learning [[paper](https://arxiv.org/abs/2106.09785)]
- Space-time Mixing Attention for Video Transformer [[paper](https://arxiv.org/abs/2106.05968)]
- Transformed CNNs: recasting pre-trained convolutional layers with self-attention [[paper](https://arxiv.org/abs/2106.05795)]
- Efficient Training of Visual Transformers with Small-Size Datasets [[paper](https://arxiv.org/abs/2106.03746)] 
- Video Instance Segmentation using Inter-Frame Communication Transformers [[paper](https://arxiv.org/abs/2106.03299)]
- Transformer in Convolutional Neural Networks [[paper](https://arxiv.org/abs/2106.03180)] [[code](https://github.com/yun-liu/TransCNN)]
- Associating Objects with Transformers for Video Object Segmentation [[paper](https://arxiv.org/abs/2106.02638)] [[code](https://github.com/z-x-yang/AOT)]
- **[TransVOS]**  TransVOS: Video Object Segmentation with Transformers [[paper](https://arxiv.org/abs/2106.00588)]
- **[KVT]** KVT: k-NN Attention for Boosting Vision Transformers [[paper](https://arxiv.org/abs/2106.00515)] 
- **[DVT]** Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length [[paper](https://arxiv.org/abs/2105.15075)]
- **[GazeTR]** Gaze Estimation using Transformer [[paper](https://arxiv.org/abs/2105.14424)] [[code](https://github.com/yihuacheng/GazeTR)]

- **[TransVOD]** End-to-End Video Object Detection with Spatial-Temporal Transformers [[paper](https://arxiv.org/abs/2105.10920)] [[code](https://github.com/SJTU-LuHe/TransVOD)]

- **[VATT]** VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text [[paper](https://arxiv.org/abs/2104.11178)]
- **[VideoGPT]** VideoGPT: Video Generation using VQ-VAE and Transformers [[paper](https://arxiv.org/abs/2104.10157)]
- Self-supervised Video Retrieval Transformer Network [[paper](https://arxiv.org/abs/2104.07993)]
- A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification [[paper](https://arxiv.org/abs/2104.01745)]
- **[Meta-DETR]** Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning [[paper](https://arxiv.org/abs/2103.11731) [[code](https://github.com/ZhangGongjie/Meta-DETR)]
- Spatiotemporal Transformer for Video-based Person Re-identification[[paper](https://arxiv.org/abs/2103.16469)] 
- Lifting Transformer for 3D Human Pose Estimation in Video [[paper](https://arxiv.org/abs/2103.14304)]
- **[TimeSformer]** Is Space-Time Attention All You Need for Video Understanding? [[paper](https://arxiv.org/abs/2102.05095)]
- **[DeepViT]** DeepViT: Towards Deeper Vision Transformer [[paper](https://arxiv.org/abs/2103.11886)] 
- Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training [[paper](https://arxiv.org/abs/2103.10043)] 
- Transformer is All You Need:
Multimodal Multitask Learning with a Unified Transformer [[paper](https://arxiv.org/abs/2102.10772)] [[code](https://mmf.sh/)]
- **[CPVT]** Do We Really Need Explicit Position Encodings for Vision Transformers? [[paper](https://arxiv.org/abs/2102.10882)] [[code](https://github.com/Meituan-AutoML/CPVT)]
- Deepfake Video Detection Using Convolutional Vision Transformer[[paper](https://arxiv.org/abs/2102.11126)]

- **[VTN]** Video Transformer Network[[paper](https://arxiv.org/abs/2102.00719)]

### 2022

**ECCV**

- **[X-CLIP]** Expanding Language-Image Pretrained Models for General Video Recognition [[paper](https://arxiv.org/abs/2208.02816)] [[code](https://aka.ms/X-CLIP)]


**CVPR**
- **[MAE]** Masked Autoencoders Are Scalable Vision Learners [[paper](https://arxiv.org/abs/2111.06377)] [[code]](https://github.com/facebookresearch/mae)
- **[BEVT]** BEVT: BERT Pretraining of Video Transformers [[paper](https://arxiv.org/pdf/2112.01529.pdf)] [[code](https://github.com/xyzforever/BEVT)]




**ICLR**
- **[RelViT]** RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning [[paper](https://arxiv.org/pdf/2204.11167.pdf)] [[code](https://github.com/NVlabs/RelViT)]
- **[CrossFormer]** CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention [[paper](https://arxiv.org/abs/2108.00154)] [[code](https://github.com/cheerss/CrossFormer)]

- Uniformer: Unified Transformer for Efficient Spatiotemporal Representation Learning [[paper](https://arxiv.org/abs/2201.04676)] [[code](https://github.com/Sense-X/UniFormer)]

- **[DAB-DETR]** DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR [[paper](https://arxiv.org/abs/2201.12329)] [[code](https://github.com/IDEA-opensource/DAB-DETR)]

### 2021
**NeurIPS**  


- **[Moment-DETR]** QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries [[paper](https://arxiv.org/abs/2107.09609)] [[code](https://github.com/jayleicn/moment_detr)]

- Associating Objects with Transformers for Video Object Segmentation [[paper](https://papers.nips.cc/paper/2021/hash/147702db07145348245dc5a2f2fe5683-Abstract.html)]

- Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers [[paper](https://papers.nips.cc/paper/2021/hash/67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html)]


**ICCV**


- A Latent Transformer for Disentangled Face Editing in Images and Videos [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.pdf)] [[code](https://github.com/InterDigitalInc/latent-transformer)]

- **[HiT]** HiT: Hierarchical Transformer With Momentum Contrast for Video-Text Retrieval [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_HiT_Hierarchical_Transformer_With_Momentum_Contrast_for_Video-Text_Retrieval_ICCV_2021_paper.pdf)]
- Event-Based Video Reconstruction Using Transformer [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Weng_Event-Based_Video_Reconstruction_Using_Transformer_ICCV_2021_paper.pdf)]
- **[STVGBert]** STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.pdf)]

- **[VidTr]** VidTr: Video Transformer Without Convolutions [[paper](https://arxiv.org/abs/2104.11746)] 

- **[ViViT]** ViViT: A Video Vision Transformer [[paper](https://arxiv.org/abs/2103.15691)]
- **[Stark]** Learning Spatio-Temporal Transformer for Visual Tracking  [[paper](https://arxiv.org/abs/2103.17154)] [[code](https://github.com/researchmm/Stark)]

-  Anticipative Video Transformer [[paper](https://arxiv.org/abs/2106.02036)] [[code](http://facebookresearch.github.io/AVT)]


**CVPR**

- **[VisTR]** End-to-End Video Instance Segmentation with Transformers [[paper](https://arxiv.org/abs/2011.14503)]


**ICML**
- Generative Video Transformer: Can Objects be the Words?  [[paper](https://arxiv.org/abs/2107.09240)]
- **[GANsformer]** Generative Adversarial Transformers [[paper](https://arxiv.org/abs/2103.01209)] [[code](https://github.com/dorarad/gansformer)]



**ACM MM**
- Video Transformer for Deepfake Detection with Incremental Learning[[paper](https://arxiv.org/abs/2108.05307)] 

- Token Shift Transformer for Video Classification  [[paper](https://arxiv.org/abs/2108.02432)] [[code](https://github.com/VideoNetworks/TokShift-Transformer)]


**MICCAI**  

- **[PNS-Net]** Progressively Normalized Self-Attention Network for Video Polyp Segmentation  [[paper](https://arxiv.org/abs/2105.08468)] [[code](https://github.com/GewelsJI/PNS-Net)]



### Other resource
- [[Awesome-Transformer-Attention](https://github.com/cmhungsteve/Awesome-Transformer-Attention)]

### Acknowledgement

Thanks the template from [Awesome-Crowd-Counting](https://github.com/gjy3035/Awesome-Crowd-Counting)





